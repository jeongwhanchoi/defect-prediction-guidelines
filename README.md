# Introduction

The goal of this repository is to create shared and agreed upon best practices for defect prediction research. Suggestions for contributions can be made by anyone in the research community by creating and issue or submitting a pull request. These guidelines is to cover the following aspects to improve the way defects prediction experiments are conducted and the results are reported. The goal is that these guidelines will cover the following aspects:

- A taxonomy that defines the different defect prediction flavors.
- How training and test data should be used for the different flavors of defect prediction.  
- Requirements on the data needed to conduct studies including quality assurance for the data. 
- Performance criteria to be used for evaluations.
- Statistical tests and visualizations.
- Reporting of results.
- Threats to validity that should be considered.
- Replication kits.

These guidelines are still a work-in-progress and not all aspects are covered yet. Once stable, we consider submitting the guidelines for review to a journal, e.g., Empirical Software Engineering and will invite everyone who significantly contributed (i.e., suggest content, helped to improve the guidelines) to co-author the manuscript. 

# Taxonomy

In the two decades, defect prediction evolved into a broad field with many different variants of how experiments are conducted. Sometimes it becomes are to distinguish between the variants - or even be aware of all of them. The goal of this section is specify fixed terminology for the different flavors of defect prediction in a taxonomy. 

- Defect Prediction
  - Release-level Defect Prediction
    - Within-Project Defect Prediction
      - Cross-Validation Experiments
      - Semi-Supervised Defect Prediction
      - Cross-Version Defect Prediction
    - Mixed-Project Defect Prediction
    - Cross-Project Defect Prediction
      - Strict Cross-Project Defect Prediction
      - Mixed Cross-Project Defect Preidction
      - Heterogeneuous Defect Prediction
  - Just-in-time Defect Prediction
 
 # Training and Test Data
 
 # Data Selection
 
 # Performance Criteria
 
 # Statistical Tests
 
 # Summary Statistics
 
 # Visualization of Results
 
 # Reporting Threats to Validity
 
 # Sharing Code and Data in Replication Kits

